---
sidebar: sidebar
permalink: hciaiedge_deploy_the_client_for_triton_inference_server_automated_deployment.html
keywords:
summary: To deploy the client for the Triton Inference Server, complete the steps listed on this page.
---

= Deploy the Client for Triton Inference Server (Automated Deployment)
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2020-09-29 18:13:43.646555
//

To deploy the client for the Triton Inference Server, complete the following steps:

. Open a VI editor,  create a deployment for the Triton client, and call the file `triton_client.yaml`.
+

....
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: triton-client
  name: triton-client
  namespace: triton
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton-client
      version: v1
  template:
    metadata:
      labels:
        app: triton-client
        version: v1
    spec:
      containers:
      - image: nvcr.io/nvidia/tritonserver:20.07- v1- py3-clientsdk
        imagePullPolicy: IfNotPresent
        name: triton-client
        resources:
          limits:
            cpu: "2"
            memory: 4Gi
          requests:
            cpu: "2"
            memory: 4Gi
....

. Deploy the client.
+

....
kubectl apply -f triton_client.yaml
....


link:hciaiedge_collect_inference_metrics_from_triton_inference_server.html[Next: Collect Inference Metrics from Triton Inference Server]
